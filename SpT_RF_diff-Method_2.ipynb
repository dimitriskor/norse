{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "50f723fd-4f3e-4d81-a68e-2959d1c1bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These receptive fields are derived from scale-space theory, specifically in the paper `Normative theory of visual receptive fields by Lindeberg, 2021 <https://www.sciencedirect.com/science/article/pii/S2405844021000025>`_.\n",
    "\n",
    "For use in spiking / binary signals, see the paper on `Translation and Scale Invariance for Event-Based Object tracking by Pedersen et al., 2023 <https://dl.acm.org/doi/10.1145/3584954.3584996>`_\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, s, c):\n",
    "    \"\"\"\n",
    "    Efficiently creates a 2d gaussian kernel.\n",
    "\n",
    "    Arguments:\n",
    "      x (torch.Tensor): A 2-d matrix\n",
    "      s (float): The variance of the gaussian\n",
    "      c (torch.Tensor): A 2x2 covariance matrix describing the eccentricity of the gaussian\n",
    "    \"\"\"\n",
    "    ci = torch.linalg.inv(c)\n",
    "    cd = torch.linalg.det(c)\n",
    "    fraction = 1 / (2 * torch.pi * s * torch.sqrt(cd))\n",
    "    b = torch.einsum(\"bimj,jk->bik\", -x.unsqueeze(2), ci)\n",
    "    a = torch.einsum(\"bij,bij->bi\", b, x)\n",
    "    return fraction * torch.exp(a / (2 * s))\n",
    "\n",
    "\n",
    "def spatial_receptive_field(\n",
    "    angle, ratio, size: int, scale: float = 2.5, dx: int = 0, dy: int = 0, domain: float = 8\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a (size x size) receptive field kernel\n",
    "\n",
    "    Arguments:\n",
    "      angle (float): The rotation of the kernel in radians\n",
    "      ratio (float): The eccentricity as a ratio\n",
    "      size (int): The size of the square kernel in pixels\n",
    "      scale (float): The scale of the field. Defaults to 2.5\n",
    "      domain (float): The initial coordinates from which the field is sampled. Defaults to 8 (equal to -8 to 8).\n",
    "    \"\"\"\n",
    "    sm = torch.ones(2)\n",
    "    sm[0] = scale\n",
    "    sm[1] = scale*ratio\n",
    "    a = torch.linspace(-domain, domain, size)\n",
    "    r = torch.ones((2,2))\n",
    "    r[0][0] = angle.cos()\n",
    "    r[0][1] = angle.sin()\n",
    "    r[1][0] = -angle.sin()\n",
    "    r[1][1] = angle.cos()    \n",
    "    c = (r * sm) @ (sm * r).T\n",
    "    xs, ys = torch.meshgrid(a, a, indexing=\"xy\")\n",
    "    coo = torch.stack([xs, ys], dim=2)\n",
    "    k = gaussian_kernel(coo, scale, c)\n",
    "    k = _derived_field(k, (dx, dy))\n",
    "    return k / k.sum()\n",
    "\n",
    "\n",
    "def _extract_derivatives(\n",
    "    derivatives: Union[int, List[Tuple[int, int]]]\n",
    ") -> Tuple[List[Tuple[int, int]], int]:\n",
    "    if isinstance(derivatives, int):\n",
    "        if derivatives == 0:\n",
    "            return [(0, 0)], 0\n",
    "        else:\n",
    "            return [\n",
    "                (x, y) for x in range(derivatives + 1) for y in range(derivatives + 1)\n",
    "            ], derivatives\n",
    "    elif isinstance(derivatives, list):\n",
    "        return derivatives, max([max(x, y) for (x, y) in derivatives])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Derivatives expected either a number or a list of tuples, but got {derivatives}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _derived_field(\n",
    "    field: torch.Tensor, derivatives: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    out = []\n",
    "    (dx, dy) = derivatives\n",
    "    if dx == 0:\n",
    "        fx = field\n",
    "    else:\n",
    "        fx = field.diff(\n",
    "            dim=0, prepend=torch.zeros(dx, field.shape[1]), n=dx)\n",
    "\n",
    "    if dy == 0:\n",
    "        fy = fx\n",
    "    else:\n",
    "        fy = fx.diff(\n",
    "            dim=1, prepend=torch.zeros(field.shape[0], dy), n=dy)\n",
    "    out.append(fy)\n",
    "    return torch.concat(out)\n",
    "\n",
    "\n",
    "def spatial_receptive_fields_with_derivatives(\n",
    "    gf_attr,\n",
    "    derivative_max: int,\n",
    "    size: int,\n",
    "    min_scale: float = 0.2,\n",
    "    max_scale: float = 1.5,\n",
    "    min_ratio: float = 0.2,\n",
    "    max_ratio: float = 1,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Creates a number of receptive field with 1st directional derivatives.\n",
    "    The parameters decide the number of combinations to scan over, i. e. the number of receptive fields to generate.\n",
    "    Specifically, we generate ``derivatives * (n_angles * n_scales * (n_ratios - 1) + n_scales)`` fields.\n",
    "    The ``(n_ratios - 1) + n_scales`` terms exist because at ``ratio = 1``, fields are perfectly symmetrical, and there\n",
    "    is therefore no reason to scan over the angles and scales for ``ratio = 1``.\n",
    "    However, ``n_scales`` receptive fields still need to be added (one for each scale-space).\n",
    "    Finally, the ``derivatives *`` term comes from the addition of spatial derivatives.\n",
    "    Arguments:\n",
    "        n_scales (int): Number of scaling combinations (the size of the receptive field) drawn from a logarithmic distribution\n",
    "        n_angles (int): Number of angular combinations (the orientation of the receptive field)\n",
    "        n_ratios (int): Number of eccentricity combinations (how \"flat\" the receptive field is)\n",
    "        size (int): The size of the square kernel in pixels\n",
    "        derivatives (Union[int, List[Tuple[int, int]]]): The spatial derivatives to include. Defaults to 0 (no derivatives).\n",
    "            Can either be a number, in which case 1 + 2 ** n derivatives will be made (except when 0, see below).\n",
    "              Example: `derivatives=0` omits derivatives\n",
    "              Example: `derivatives=1` provides 2 spatial derivatives + 1 without derivation\n",
    "            Or a list of tuples specifying the derivatives in both spatial dimensions\n",
    "              Example: `derivatives=[(0, 0), (1, 2)]` provides two outputs, one without derivation and one :math:`\\partial_x \\partial^2_y`\n",
    "    \"\"\"\n",
    "\n",
    "    def _stack_empty(x):\n",
    "        if len(x) == 0:\n",
    "            return torch.tensor([])\n",
    "        else:\n",
    "            return torch.stack(x)\n",
    "\n",
    "    # We add extra space in both the domain and size to account for the derivatives\n",
    "    domain = 8 + derivative_max * size * 0.5\n",
    "\n",
    "    rings = _stack_empty(\n",
    "        [\n",
    "            spatial_receptive_field(attr[1], attr[2], size=size + 2 * derivative_max, scale=attr[0], dx=attr[3], dy=attr[4], domain=domain)\n",
    "            for attr in gf_attr\n",
    "        ]\n",
    "    )\n",
    "    derived_fields = rings[\n",
    "        :,\n",
    "        derivative_max : size + derivative_max,\n",
    "        derivative_max : size + derivative_max,  # Remove extra space\n",
    "    ]\n",
    "    #derived_fields.sum().backward()\n",
    "    #print(angles.grad)\n",
    "    return derived_fields\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6af83a25-7d29-4ea4-aaaf-103e56228732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These receptive fields are derived from scale-space theory, specifically in the paper `Normative theory of visual receptive fields by Lindeberg, 2021 <https://www.sciencedirect.com/science/article/pii/S2405844021000025>`_.\n",
    "\n",
    "For use in spiking / binary signals, see the paper on `Translation and Scale Invariance for Event-Based Object tracking by Pedersen et al., 2023 <https://dl.acm.org/doi/10.1145/3584954.3584996>`_\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable, List, NamedTuple, Optional, Tuple, Type, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "#from norse.torch.module.leaky_integrator_box import LIBoxCell, LIBoxParameters\n",
    "from norse.torch.module.snn import SNNCell\n",
    "\n",
    "\n",
    "\n",
    "class SpatialReceptiveField2d(torch.nn.Module):\n",
    "    \"\"\"Creates a spatial receptive field as 2-dimensional convolutions.\n",
    "    The parameters decide the number of combinations to scan over, i. e. the number of receptive fields to generate.\n",
    "    Specifically, we generate ``n_scales * n_angles * (n_ratios - 1) + n_scales`` output_channels with aggregation,\n",
    "    and ``in_channels * (n_scales * n_angles * (n_ratios - 1) + n_scales)`` without aggregation.\n",
    "\n",
    "    The ``(n_ratios - 1) + n_scales`` terms exist because at ``ratio = 1``, fields are perfectly symmetrical, and there\n",
    "    is therefore no reason to scan over the angles and scales for ``ratio = 1``.\n",
    "    However, ``n_scales`` receptive field still needs to be added (one for each scale-space).\n",
    "\n",
    "    Parameters:\n",
    "        n_scales (int): Number of scaling combinations (the size of the receptive field) drawn from a logarithmic distribution\n",
    "        n_angles (int): Number of angular combinations (the orientation of the receptive field)\n",
    "        n_ratios (int): Number of eccentricity combinations (how \"flat\" the receptive field is)\n",
    "        size (int): The size of the square kernel in pixels\n",
    "        derivatives (Union[int, List[Tuple[int, int]]]): The number of derivatives to use in the receptive field.\n",
    "        aggregate (bool): If True, sums the input channels over all output channels. If False, every\n",
    "        output channel is mapped to every input channel, which may blow up in complexity.\n",
    "        **kwargs: Arguments passed on to the underlying torch.nn.Conv2d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        n_scales: int,\n",
    "        n_angles: int,\n",
    "        n_ratios: int,\n",
    "        size: int,\n",
    "        derivatives: Union[int, List[Tuple[int, int]]] = 0,\n",
    "        min_scale: float = 0.2,\n",
    "        max_scale: float = 1.5,\n",
    "        min_ratio: float = 0.2,\n",
    "        max_ratio: float = 1,\n",
    "        aggregate: bool = True,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.aggregate = aggregate\n",
    "        self.size = size\n",
    "        self.derivatives = derivatives\n",
    "        self.in_channels = in_channels\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self.angles = torch.linspace(0, torch.pi - torch.pi / n_angles, n_angles, requires_grad=True)\n",
    "        self.ratios = torch.linspace(min_ratio, max_ratio, n_ratios)\n",
    "        self.log_scales = torch.linspace(min_scale, max_scale, n_scales)\n",
    "        scales = torch.exp(self.log_scales)\n",
    "        \n",
    "        self.update = False\n",
    "        derivative_list, derivative_max = _extract_derivatives(self.derivatives)\n",
    "        gf_attr = [[s, a, r, d[0], d[1]] for s in scales for a in self.angles for r in self.ratios for d in derivative_list]\n",
    "        self.gf_attr = torch.tensor(gf_attr, requires_grad=True)\n",
    "        self.fields = spatial_receptive_fields_with_derivatives(\n",
    "            gf_attr,\n",
    "            derivative_max,\n",
    "            size,\n",
    "        )\n",
    "        if self.aggregate:\n",
    "            self.out_channels = self.fields.shape[0]\n",
    "            weights = self.fields.unsqueeze(1).repeat(1, in_channels, 1, 1)\n",
    "        else:\n",
    "            self.out_channels = self.fields.shape[0] * in_channels\n",
    "            empty_weights = torch.zeros(in_channels, self.fields.shape[0], size, size)\n",
    "            weights = []\n",
    "            for i in range(in_channels):\n",
    "                in_weights = empty_weights.clone()\n",
    "                in_weights[i] = self.fields\n",
    "                weights.append(in_weights)\n",
    "            weights = torch.concat(weights, 1).permute(1, 0, 2, 3)\n",
    "        self.conv = torch.nn.Conv2d(in_channels, self.out_channels, size, **kwargs)\n",
    "        self.conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
    "        self.conv.weight[:] = weights[:]\n",
    "        print(self.gf_attr)\n",
    "        print(self.angles, self.ratios, self.log_scales)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.update:\n",
    "            scales = torch.exp(self.log_scales)\n",
    "            self.update = False\n",
    "            self.fields = spatial_receptive_fields_with_derivatives(\n",
    "                scales,\n",
    "                self.angles,\n",
    "                self.ratios,\n",
    "                self.size,\n",
    "                self.derivatives,\n",
    "            )\n",
    "            if self.aggregate:\n",
    "                self.out_channels = self.fields.shape[0]\n",
    "                weights = self.fields.unsqueeze(1).repeat(1, self.in_channels, 1, 1)\n",
    "            else:\n",
    "                self.out_channels = self.fields.shape[0] * self.in_channels\n",
    "                empty_weights = torch.zeros(self.in_channels, self.fields.shape[0], self.size, self.size)\n",
    "                weights = []\n",
    "                for i in range(self.in_channels):\n",
    "                    in_weights = empty_weights.clone()\n",
    "                    in_weights[i] = self.fields\n",
    "                    weights.append(in_weights)\n",
    "                weights = torch.concat(weights, 1).permute(1, 0, 2, 3)\n",
    "            self.conv = torch.nn.Conv2d(self.in_channels, self.out_channels, self.size, **self.kwargs)\n",
    "            self.conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
    "            self.conv.weight[:] = weights[:]\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\n",
    "def diff(model, lr = 0.01):\n",
    "    with torch.no_grad():\n",
    "        model.angles += model.angles.grad*lr\n",
    "        model.ratios += model.ratios.grad*lr\n",
    "        model.log_scales += model.log_scales.grad*lr\n",
    "    model.angles.grad.zero_()\n",
    "    model.log_scales.grad.zero_()\n",
    "    model.ratios.grad.zero_()\n",
    "    model.update = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "09ce6d87-f7dc-4154-9119-4b01737a3177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2214, 0.0000, 0.2000, 0.0000, 0.0000],\n",
      "        [1.2214, 0.0000, 1.0000, 0.0000, 0.0000]], requires_grad=True)\n",
      "tensor([0.], requires_grad=True) tensor([0.2000, 1.0000]) tensor([0.2000])\n",
      "tensor([[1.2214, 0.0000, 0.2000, 0.0000, 0.0000],\n",
      "        [1.2214, 0.0000, 1.0000, 0.0000, 0.0000]], requires_grad=True) \n",
      " None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(rf\u001b[38;5;241m.\u001b[39mgf_attr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, rf\u001b[38;5;241m.\u001b[39mgf_attr\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf\u001b[38;5;241m.\u001b[39mangles, rf\u001b[38;5;241m.\u001b[39mlog_scales, rf\u001b[38;5;241m.\u001b[39mratios)\n",
      "Cell \u001b[0;32mIn[128], line 126\u001b[0m, in \u001b[0;36mdiff\u001b[0;34m(model, lr)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    125\u001b[0m     model\u001b[38;5;241m.\u001b[39mangles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mangles\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m*\u001b[39mlr\n\u001b[0;32m--> 126\u001b[0m     model\u001b[38;5;241m.\u001b[39mratios \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratios\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlr\u001b[49m\n\u001b[1;32m    127\u001b[0m     model\u001b[38;5;241m.\u001b[39mlog_scales \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlog_scales\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m*\u001b[39mlr\n\u001b[1;32m    128\u001b[0m model\u001b[38;5;241m.\u001b[39mangles\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #print(rf.shape)\n",
    "# for a in rf.conv.weight:\n",
    "#     #plt.figure()\n",
    "#     plt.imshow(a[0].detach().numpy())\n",
    "\n",
    "rf = SpatialReceptiveField2d(1, 1, 1, 2, 9)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = True\n",
    "target = spatial_receptive_field(torch.tensor(1.), torch.tensor(.2), torch.tensor(9), torch.tensor(2.5))\n",
    "inp = torch.ones((1, 9,9))\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    start = time.time()\n",
    "    out = rf(inp)\n",
    "    end = time.time() - start\n",
    "    l = loss(target, sum(sum(out)))\n",
    "    l.backward()\n",
    "    print(rf.gf_attr, \"\\n\", rf.gf_attr.grad)\n",
    "    diff(rf, 1e7)\n",
    "    \n",
    "print(rf.angles, rf.log_scales, rf.ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed71c72-1fb4-4063-be8e-4887e97fc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sum(out[:]).detach().numpy()))\n",
    "for i in rf.conv.weight:\n",
    "    plt.figure()\n",
    "    plt.imshow(((i[0].detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "aea4dd2b-ad2c-4acd-a31e-82235df5e43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef5c93b940>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "db33f9bb-5320-4c78-aedb-6b9c0ff26254",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0653d2a1-03ed-4b4e-b8c9-57a2c94977a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "144be25d-e021-46c7-a970-f9d317bbda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r grad: tensor([[ 2.3669e-04, -1.7095e-05],\n",
      "        [ 1.6632e-01, -3.8603e-03]])\n",
      "angle grad: None\n",
      "ratio grad: None\n",
      "scale grad: tensor(-0.0365)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def gaussian_kernel(x, s, c):\n",
    "    \"\"\"\n",
    "    Efficiently creates a 2d gaussian kernel.\n",
    "\n",
    "    Arguments:\n",
    "      x (torch.Tensor): A 2-d matrix\n",
    "      s (float): The variance of the gaussian\n",
    "      c (torch.Tensor): A 2x2 covariance matrix describing the eccentricity of the gaussian\n",
    "    \"\"\"\n",
    "    ci = torch.linalg.inv(c)\n",
    "    cd = torch.linalg.det(c)\n",
    "    fraction = 1 / (2 * torch.pi * s * torch.sqrt(cd))\n",
    "    b = torch.einsum(\"bimj,jk->bik\", -x.unsqueeze(2), ci)\n",
    "    a = torch.einsum(\"bij,bij->bi\", b, x)\n",
    "    return fraction * torch.exp(a / (2 * s))\n",
    "\n",
    "s = torch.tensor(2., requires_grad=True)\n",
    "ratio = torch.tensor(.2, requires_grad=True)\n",
    "sm = torch.tensor([s, s * ratio], requires_grad=True)\n",
    "x = torch.linspace(-5, 5, 11)\n",
    "x = torch.unsqueeze(x, 0).repeat(11, 1)\n",
    "x = torch.stack((x, x.T))\n",
    "x = torch.einsum(\"abc->bca\",x)\n",
    "angle = torch.tensor(np.pi/3, requires_grad=True)\n",
    "r = torch.tensor(\n",
    "    [[torch.cos(angle), torch.sin(angle)], [-torch.sin(angle), torch.cos(angle)]],\n",
    "    dtype=torch.float32, requires_grad=True\n",
    ")\n",
    "\n",
    "c = (r * sm) @ (sm * r).T\n",
    "\n",
    "a = gaussian_kernel(x, s, c)\n",
    "plt.imshow(a.detach().numpy())\n",
    "a.sum().backward()\n",
    "\n",
    "print(\"r grad:\", r.grad)\n",
    "print(\"angle grad:\", angle.grad)\n",
    "print(\"ratio grad:\", ratio.grad)\n",
    "print(\"scale grad:\", s.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5835ed23-7aa4-418d-89f7-84fe196efb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MmBackward0 object at 0x7f1f1441c3d0>\n"
     ]
    }
   ],
   "source": [
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9785f3c2-11f8-4429-a6a3-5e3cba785b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "angle = torch.tensor(np.pi/3, requires_grad=True)\n",
    "r = torch.tensor([[torch.cos(angle), torch.sin(angle)], [-torch.sin(angle), torch.cos(angle)]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "r.sum().backward()\n",
    "print(angle.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "403b7199-cc3e-4a26-9ce0-eda79b86d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "angle = torch.tensor(np.pi/3, requires_grad=True)\n",
    "b=torch.tensor([torch.sin(angle)], dtype=torch.float32, requires_grad=True)\n",
    "b.sum().backward()\n",
    "print(b.grad)\n",
    "print(angle.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2aa0b9b-97a2-4f77-b452-c492c1850e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is angle a leaf variable? True\n",
      "Is b a leaf variable? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "angle = torch.tensor(np.pi/3, requires_grad=True)\n",
    "b = torch.tensor([angle.sin()], dtype=torch.float32)\n",
    "\n",
    "# Use the in-place sin_ operation\n",
    "b.sin_()\n",
    "\n",
    "print(\"Is angle a leaf variable?\", angle.is_leaf)  # True\n",
    "print(\"Is b a leaf variable?\", b.is_leaf)          # False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "666a2f48-7286-4d19-b15b-a93bbe661584",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = torch.tensor([np.pi/3, 1], requires_grad=True)\n",
    "ratio = torch.tensor([np.pi/3, 1], requires_grad=True)\n",
    "scale = torch.tensor([np.pi/3, 1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd8f6188-20fa-4dfc-b71c-ff1cdd93fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.2705e-11, -3.0393e-12])\n"
     ]
    }
   ],
   "source": [
    "spatial_receptive_fields_with_derivatives(scale, angle, ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53afa05a-c2ec-43d4-bd25-4be14e06d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Method 2\n",
    "\n",
    "\"\"\"\n",
    "These receptive fields are derived from scale-space theory, specifically in the paper `Normative theory of visual receptive fields by Lindeberg, 2021 <https://www.sciencedirect.com/science/article/pii/S2405844021000025>`_.\n",
    "\n",
    "For use in spiking / binary signals, see the paper on `Translation and Scale Invariance for Event-Based Object tracking by Pedersen et al., 2023 <https://dl.acm.org/doi/10.1145/3584954.3584996>`_\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable, List, NamedTuple, Optional, Tuple, Type, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "#from norse.torch.module.leaky_integrator_box import LIBoxCell, LIBoxParameters\n",
    "from norse.torch.module.snn import SNNCell\n",
    "\n",
    "\n",
    "\n",
    "class SpatialReceptiveField2d(torch.nn.Module):\n",
    "    \"\"\"Creates a spatial receptive field as 2-dimensional convolutions.\n",
    "    The parameters decide the number of combinations to scan over, i. e. the number of receptive fields to generate.\n",
    "    Specifically, we generate ``n_scales * n_angles * (n_ratios - 1) + n_scales`` output_channels with aggregation,\n",
    "    and ``in_channels * (n_scales * n_angles * (n_ratios - 1) + n_scales)`` without aggregation.\n",
    "\n",
    "    The ``(n_ratios - 1) + n_scales`` terms exist because at ``ratio = 1``, fields are perfectly symmetrical, and there\n",
    "    is therefore no reason to scan over the angles and scales for ``ratio = 1``.\n",
    "    However, ``n_scales`` receptive field still needs to be added (one for each scale-space).\n",
    "\n",
    "    Parameters:\n",
    "        n_scales (int): Number of scaling combinations (the size of the receptive field) drawn from a logarithmic distribution\n",
    "        n_angles (int): Number of angular combinations (the orientation of the receptive field)\n",
    "        n_ratios (int): Number of eccentricity combinations (how \"flat\" the receptive field is)\n",
    "        size (int): The size of the square kernel in pixels\n",
    "        derivatives (Union[int, List[Tuple[int, int]]]): The number of derivatives to use in the receptive field.\n",
    "        aggregate (bool): If True, sums the input channels over all output channels. If False, every\n",
    "        output channel is mapped to every input channel, which may blow up in complexity.\n",
    "        **kwargs: Arguments passed on to the underlying torch.nn.Conv2d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        n_scales: int,\n",
    "        n_angles: int,\n",
    "        n_ratios: int,\n",
    "        size: int,\n",
    "        derivatives: Union[int, List[Tuple[int, int]]] = 0,\n",
    "        min_scale: float = 0.2,\n",
    "        max_scale: float = 1.5,\n",
    "        min_ratio: float = 0.2,\n",
    "        max_ratio: float = 1,\n",
    "        aggregate: bool = True,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.angles = torch.linspace(0, torch.pi - torch.pi / n_angles, n_angles, requires_grad=True)\n",
    "        self.ratios = torch.linspace(min_ratio, max_ratio, n_ratios, requires_grad=True)\n",
    "        self.log_scales = torch.linspace(min_scale, max_scale, n_scales, requires_grad=True)\n",
    "        self.scales = torch.exp(self.log_scales)\n",
    "        self.size = size\n",
    "        self.derivatives = derivatives\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale \n",
    "        self.min_ratio = min_ratio\n",
    "        self.max_ratio = max_ratio\n",
    "        self.aggregate = aggregate\n",
    "        self.in_channels = in_channels\n",
    "        self.kwargs = kwargs \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        fields = spatial_receptive_fields_with_derivatives(\n",
    "            self.scales,\n",
    "            self.angles,\n",
    "            self.ratios,\n",
    "            self.size,\n",
    "            self.derivatives,\n",
    "            self.min_scale,\n",
    "            self.max_scale,\n",
    "            self.min_ratio,\n",
    "            self.max_ratio,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        if self.aggregate:\n",
    "            self.out_channels = fields.shape[0]\n",
    "            weights = fields.unsqueeze(1).repeat(1, self.in_channels, 1, 1)\n",
    "        else:\n",
    "            self.out_channels = fields.shape[0] * in_channels\n",
    "            empty_weights = torch.zeros(in_channels, fields.shape[0], self.size, self.size)\n",
    "            weights = []\n",
    "            for i in range(in_channels):\n",
    "                in_weights = empty_weights.clone()\n",
    "                in_weights[i] = fields\n",
    "                weights.append(in_weights)\n",
    "            weights = torch.concat(weights, 1).permute(1, 0, 2, 3)\n",
    "        self.conv = torch.nn.Conv2d(self.in_channels, self.out_channels, self.size, **self.kwargs)\n",
    "        self.conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
    "        self.conv.weight[:] = weights[:]\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0fee4-cace-4fad-97a2-193a597f2088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck",
   "language": "python",
   "name": "speck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
